# Spark Cluster in Docker with Dataset Transformation

This project demonstrates how to deploy a standalone Apache Spark cluster (single-node) using Docker and perform transformations on a dataset using PySpark.

## Project Overview

1. **Deploy Spark Cluster**: A standalone, single-node Apache Spark cluster is deployed using Docker.
2. **Load Dataset**: The dataset [CareerBuilder Job Listing 2020](https://www.kaggle.com/promptcloud/careerbuilder-job-listing-2020) is loaded into the Spark cluster.
3. **Data Transformation**: Transformations are applied to the dataset using PySpark and Jupiter Notebook.

## Prerequisites

- Docker and Docker Compose installed on your local machine.

## Getting Started
run run_me.bat file
